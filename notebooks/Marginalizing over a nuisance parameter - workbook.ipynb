{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from astropy.io import ascii\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('apw-notebook.mplstyle')\n",
    "%matplotlib inline\n",
    "from scipy.stats import scoreatpercentile\n",
    "import emcee\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll subclass this helper class below to define our probabilistic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ProbModel(object):\n",
    "    \n",
    "    def __init__(self, x, y, y_err):\n",
    "        \"\"\" \n",
    "        We store the data as attributes of the object so we don't have to \n",
    "        keep passing it in to the methods that compute the probabilities.\n",
    "        \"\"\"\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.y_err = np.asarray(y_err)\n",
    "\n",
    "    def ln_likelihood(self, pars):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def ln_prior(self, pars):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def ln_posterior(self, pars):\n",
    "        \"\"\" \n",
    "        Up to a normalization constant, the log of the posterior pdf is just \n",
    "        the sum of the log likelihood plus the log prior.\n",
    "        \"\"\"\n",
    "        lnp = self.ln_prior(pars)\n",
    "        if np.isinf(lnp): # short-circuit if the prior is infinite (don't bother computing likelihood)\n",
    "            return lnp\n",
    "\n",
    "        lnL = self.ln_likelihood(pars).sum()\n",
    "        lnprob = lnp + lnL\n",
    "\n",
    "        if np.isnan(lnprob):\n",
    "            return -np.inf\n",
    "\n",
    "        return lnprob\n",
    "    \n",
    "    def __call__(self, pars):\n",
    "        return self.ln_posterior(pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Marginalization in simple probabilistic models\n",
    "\n",
    "In both cases below, let's assume we are given $N$ measurements of the flux of a source, $f_n$, at times $t_n$ and are handed Gaussian uncertainties $\\sigma_n$ for each datum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Case 1\n",
    "\n",
    "We would like to measure the mean or true flux of the source, $f_0$. But when we plot the data, we notice that the scatter in the flux measurements looks much larger than the reported uncertainties. The error bars are either *underestimated*, or the source has some *intrinsic scatter* or noise that we need to take into account in our model. (BTW: In practice, disentangling these two possibilities is impossible without other data.) We'll build a model that adds this extra scatter (whether it's because of underestimated error bars or intrinsic scatter) under the assumption that the unaccounted for noise is also Gaussian. Then, we'll want to marginalize our posterior pdf over the nuisance parameter (the extra variance).\n",
    "\n",
    "If the extra noise is correlated in some way (e.g., stellar turbulence for light curves of stars, AGN variability, nuisance features in a spectrum) then this is *not* an unbiased way to infer the mean model for the source.\n",
    "\n",
    "Let's first read some data (also in the repo) that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tbl1 = ascii.read(\"case1.csv\")\n",
    "tbl1.colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first thing we can do is just estimate the mean flux and uncertainty on the mean using a maximum-likelihood estimator (ignoring the extra variance). We find that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flux_ivar = 1 / tbl1['flux_err']**2 # inverse-variance\n",
    "mean_flux = np.sum(tbl1['flux']*flux_ivar) / np.sum(flux_ivar)\n",
    "mean_flux_err = np.sqrt(1 / np.sum(flux_ivar))\n",
    "\n",
    "# the truth is 10.\n",
    "print('Mean flux: {:.2f} ± {:.2f}'.format(mean_flux, mean_flux_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Clearly the uncertainty we get on the mean flux is too small! This estimate is many sigma away from the truth (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(tbl1['time'], tbl1['flux'], tbl1['flux_err'], \n",
    "             linestyle='none', marker='o')\n",
    "plt.axhline(mean_flux, linestyle='--')\n",
    "\n",
    "xlim = plt.xlim()\n",
    "plt.axhspan(mean_flux-mean_flux_err, mean_flux+mean_flux_err, \n",
    "            xmin=xlim[0], xmax=xlim[1], color='#aaaaaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll now instead add a parameter to our model to account for this intrinsic scatter: a variance $V$. Following our derivation from the board, our likelihood (for a single observation) with this extra parameter is:\n",
    "\n",
    "$$\n",
    "p(f_n \\,|\\, f_0, V, \\sigma_n^2) = \\mathcal{N}(f_n \\,|\\, f_0, \\sigma_n^2 + V) \\\\\n",
    "\\ln p(f_n \\,|\\, f_0, V, \\sigma_n^2) = -\\frac{1}{2}\\frac{(f_n - f_0)^2}{\\sigma_n^2 + V} - \n",
    "\\frac{1}{2}\\ln\\left[2\\pi \\, (\\sigma_n^2 + V) \\right]\n",
    "$$\n",
    "\n",
    "If we want to use MCMC and do Bayesian inference, we also need to specify prior probability distributions for both $f_0$ and $V$. For $f_0$, we'll use a uniform prior over some large range of positive values (say, from 0 to 100). For the extra variance $V$, we'll use a prior that is uniform in log-space over some large range, e.g.:\n",
    "\n",
    "$$\n",
    "p(\\ln V) = \\mathcal{U}(-2, 2)\n",
    "$$\n",
    "\n",
    "This choice comes from using a [Jeffrey's prior](https://en.wikipedia.org/wiki/Jeffreys_prior). When using a log-uniform prior like the above, it is usually easier to just sample in the log of the variable. That is, we'll use $\\ln V$ as our parameter instead of $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model1(ProbModel):\n",
    "\n",
    "    def ln_prior(self, pars):\n",
    "        f0, lnV = pars\n",
    "        \n",
    "        # FILL IN HERE\n",
    "    \n",
    "    def ln_likelihood(self, pars):\n",
    "        f0, lnV = pars\n",
    "        \n",
    "        # FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model1 = Model1(x=tbl1['time'], y=tbl1['flux'], y_err=tbl1['flux_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_walkers = 16\n",
    "n_dim = 2\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generate initial conditions for the sampler\n",
    "p0 = [1., 0.5]\n",
    "p0 = emcee.utils.sample_ball(p0, np.full_like(p0, 1E-3), size=n_walkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos,_,_ = sampler.run_mcmc(p0, 1024) # burn-in phase\n",
    "sampler.reset() # throw out the burn-in samples\n",
    "_ = sampler.run_mcmc(pos, 4096) # re-run from position at end of burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(sampler.flatchain, \n",
    "                    labels=['$f_0$', r'$\\ln V$'],\n",
    "                    truths=[10., np.log(0.5**2)])\n",
    "# fig.axes[0].axvline(mean_flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The marginalization we'd like to do is, defining $a = ln V$:\n",
    "\n",
    "$$\n",
    "p(f_0 \\,|\\, \\{f_n\\}) = \\int {\\rm d}a \\, p(f_0, a \\,|\\, \\{f_n\\})\n",
    "$$\n",
    "\n",
    "From running MCMC, we have samples from the posterior pdf $p(f_0, a \\,|\\, \\{f_n\\})$. To get samples from the marginal distribution, it turns out all we need to do is ignore the column of values for $\\ln V$! That is, MCMC gives us samples but also implicitly does the margnializations we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(tbl1['time'], tbl1['flux'], tbl1['flux_err'], \n",
    "             linestyle='none', marker='o')\n",
    "\n",
    "xlim = plt.xlim()\n",
    "plt.axhline(10, color='k', zorder=0)\n",
    "\n",
    "# mean flux\n",
    "f0_median = np.median(sampler.flatchain[:,0])\n",
    "f0_quantiles = scoreatpercentile(sampler.flatchain[:,0], [16, 84])\n",
    "plt.axhline(f0_median, color='#3182bd', zorder=-1)\n",
    "plt.axhspan(f0_quantiles[0], f0_quantiles[1], \n",
    "            xmin=xlim[0], xmax=xlim[1], color='#3182bd', alpha=0.25, zorder=-10)\n",
    "\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$f$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f0_samples = sampler.flatchain[:,0]\n",
    "print('MCMC estimated f_0: {:.2f} ± {:.2f}'.format(np.median(f0_samples), np.std(f0_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Case 2\n",
    "\n",
    "We again would like to measure the mean or true flux $f_0$ of a source under the assumption that the source is not varying. However, we have some reason to believe that there was a problem with some fraction of our data and there will be outliers. Here we'll construct a model to handle this situation.\n",
    "\n",
    "Let's first read some data (also in the repo) that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tbl2 = ascii.read(\"case2.csv\")\n",
    "tbl2.colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll again estimate the mean flux and uncertainty on the mean using a simple maximum-likelihood estimator, ignoring the issue of outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flux_ivar = 1 / tbl2['flux_err']**2 # inverse-variance\n",
    "mean_flux = np.sum(tbl2['flux']*flux_ivar) / np.sum(flux_ivar)\n",
    "mean_flux_err = np.sqrt(1 / np.sum(flux_ivar))\n",
    "\n",
    "# the truth is 10.\n",
    "print('Mean flux: {:.2f} ± {:.2f}'.format(mean_flux, mean_flux_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The true flux is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(tbl2['time'], tbl2['flux'], tbl2['flux_err'], \n",
    "             linestyle='none', marker='o')\n",
    "plt.axhline(mean_flux, linestyle='--')\n",
    "\n",
    "xlim = plt.xlim()\n",
    "plt.axhspan(mean_flux-mean_flux_err, mean_flux+mean_flux_err, \n",
    "            xmin=xlim[0], xmax=xlim[1], color='#aaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model2(ProbModel):\n",
    "\n",
    "    def ln_prior(self, pars):\n",
    "        f0, Q, out_mu, out_lnV = pars\n",
    "        \n",
    "        # FILL IN HERE\n",
    "    \n",
    "    def ln_likelihood_inlier(self, pars):\n",
    "        f0, Q, _, _ = pars\n",
    "        \n",
    "        # FILL IN HERE\n",
    "    \n",
    "    def ln_likelihood_outlier(self, pars):\n",
    "        _, Q, out_mu, out_lnV = pars\n",
    "        \n",
    "        # FILL IN HERE\n",
    "        \n",
    "    def ln_likelihood(self, pars):       \n",
    "        # the outlier likelihood:\n",
    "        ll_out = self.ln_likelihood_outlier(pars)\n",
    "\n",
    "        # the inlier likelihood:\n",
    "        ll_in = self.ln_likelihood_inlier(pars)\n",
    "\n",
    "        # Combine these using log-add-exp for numerical stability.\n",
    "        ll = np.sum(np.logaddexp(ll_out, ll_in))\n",
    "\n",
    "        return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model2 = Model2(x=tbl2['time'], y=tbl2['flux'], y_err=tbl2['flux_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_walkers = 32\n",
    "n_dim = 4\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generate initial conditions\n",
    "p0 = [10., 0.1, 10., 0.]\n",
    "p0 = emcee.utils.sample_ball(p0, np.full_like(p0, 1E-3), size=n_walkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos,_,_ = sampler.run_mcmc(p0, 1024)\n",
    "sampler.reset()\n",
    "_ = sampler.run_mcmc(pos, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for walker in sampler.chain[...,3]:\n",
    "    plt.plot(walker, marker='', drawstyle='steps-mid', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(sampler.flatchain, \n",
    "                    labels=['$f_0$', '$Q$', r'$\\mu_{\\rm bad}$', r'$\\ln V_{\\rm bad}$'],\n",
    "                    truths=[10., 0.75, np.nan, np.nan]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the things it looks like we lose by marginalizing over the per-observation outlier flag (the $q_n$'s) is the ability to identify likely outlier points. We can reconstruct this after the fact using the samples above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sampler.flatchain.shape[0]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "K = 65536\n",
    "post_prob = np.zeros(len(tbl2))\n",
    "for i in range(K): # only use some of the samples\n",
    "    theta = sampler.flatchain[i]\n",
    "    ll_in = model2.ln_likelihood_inlier(theta)\n",
    "    ll_out = model2.ln_likelihood_outlier(theta)\n",
    "    post_prob += np.exp(ll_in - np.logaddexp(ll_in, ll_out))\n",
    "\n",
    "post_prob /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\", \".join(map(\"{0:.3f}\".format, 1-post_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "outlier_idx = post_prob < 0.5 # <50% chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(tbl2['time'][~outlier_idx], tbl2['flux'][~outlier_idx], tbl2['flux_err'][~outlier_idx], \n",
    "             linestyle='none', marker='o')\n",
    "plt.errorbar(tbl2['time'][outlier_idx], tbl2['flux'][outlier_idx], tbl2['flux_err'][outlier_idx], \n",
    "             linestyle='none', marker='o', color='r')\n",
    "\n",
    "xlim = plt.xlim()\n",
    "\n",
    "f0_median = np.median(sampler.flatchain[:,0])\n",
    "f0_quantiles = scoreatpercentile(sampler.flatchain[:,0], [16, 84])\n",
    "plt.axhline(f0_median, color='#3182bd', zorder=-1)\n",
    "plt.axhspan(f0_quantiles[0], f0_quantiles[1], \n",
    "            xmin=xlim[0], xmax=xlim[1], color='#3182bd', alpha=0.25, zorder=-10)\n",
    "\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$f$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
