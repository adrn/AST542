{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import glob\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('apw-notebook.mplstyle')\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from astropy.table import Table\n",
    "\n",
    "from astroML.datasets.tools import get_data_home\n",
    "from astroML.dimensionality import iterative_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I forgot to have you do this -- execute the cell below this one immediately so it can run while I blabber...__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spec_iterative_pca(outfile, n_ev=16, n_iter=32, norm='L2'):\n",
    "    \"\"\"\n",
    "    This function takes the file outputted above, performs an iterative\n",
    "    PCA to fill in the gaps, and appends the results to the same file.\n",
    "    \"\"\"\n",
    "    data_in = np.load(outfile)\n",
    "    spectra = data_in['spectra']\n",
    "    mask = data_in['mask']\n",
    "\n",
    "    res = iterative_pca(spectra, mask,\n",
    "                        n_ev=n_ev, n_iter=n_iter, norm=norm,\n",
    "                        full_output=True)\n",
    "\n",
    "    input_dict = dict([(key, data_in[key]) for key in data_in.files])\n",
    "\n",
    "    # don't save the reconstructed spectrum: this can easily\n",
    "    # be recomputed from the other parameters.\n",
    "    input_dict['mu'] = res[1]\n",
    "    input_dict['evecs'] = res[2]\n",
    "    input_dict['evals'] = res[3]\n",
    "    input_dict['norms'] = res[4]\n",
    "    input_dict['coeffs'] = res[5]\n",
    "\n",
    "    np.savez(outfile,\n",
    "             **input_dict)\n",
    "    \n",
    "spec_iterative_pca(path.join(get_data_home(), sdss_corrected_spectra.ARCHIVE_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Astronomical data is often \"big data\" in two ways: many objects _and_ many features. For example, a spectroscopic survey -- you can think of the fluxes measured at each wavelength as being individual features! In that context, each object or data point could have ~1000's of features. With data that is this high-dimensional, how do you visualize it? How do you know where the information is? \n",
    "\n",
    "We know from other work that stellar spectra can be characterized typically by just a few parameters, things like $T_{\\rm eff}$, $\\log g$, $[{\\rm Fe}/{\\rm H}]$, etc. This suggests that there is a mapping from \"fluxes\" to some much lower-dimensional space that preserves most of the information. You might call this \"learning the parameters of a model,\" \"data compression,\" or \"feature extraction\" (or all). The goal of dimensionality reduction is to approximate this transformation while preserving most of the _information_. PCA is a particular dimensionality reduction method that uses a linear transformation for this step. PCA therefore corresponds to projecting the data onto a set of vectors aligned with the directions of maximum _variance_ in the data.\n",
    "\n",
    "Why does more variance correspond to more information? Imagine 2D distribution of points aligned with axes with large variance in one direction, small variance in the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 1., size=1024)\n",
    "y = np.random.normal(0, 0.1, size=x.size)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y, alpha=0.25, marker='.')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature $x_2$ is not very informative, i.e. the data have a small variance in this dimension. We would therefore preserve most of the information about the data by ignoring this dimension and just using $x_1$ in whatever classification problem or analysis we do with the data. For small numbers of dimensions, reducing the dimensionality doesn't help much in analysis, visualization, storage, etc. But for large numbers of dimensions, reducing the dimensionality can be crucial for interpretability or making problems tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "_Disclaimer: There are a number of ways to interpret or present the concepts behind PCA -- this is just the way I think of it!_\n",
    "\n",
    "The \"principal components\" of a dataset are the eigenvectors of the empirical covariance matrix of the data ordered by the magnitude of the corresponding eigenvalues. As a procedure, PCA is just a projection of the original data onto a subspace defined by a subset of the eigenvectors. Let's unpack that a bit...\n",
    "\n",
    "Let's imagine we observe a few quantities (features) that end up being highly correlated, say, the size and the absolute magnitude of a bunch of galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.loadtxt('../data/correlated_data.csv', delimiter=',')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 2, figsize=(6,6), sharex=True, sharey=True)\n",
    "\n",
    "axes[0,0].plot(X[:,0], X[:,1], marker='.', alpha=0.5, linestyle='none')\n",
    "axes[1,0].plot(X[:,0], X[:,2], marker='.', alpha=0.5, linestyle='none')\n",
    "axes[1,1].plot(X[:,1], X[:,2], marker='.', alpha=0.5, linestyle='none')\n",
    "\n",
    "axes[0,0].set_ylabel('x2')\n",
    "axes[1,0].set_ylabel('x3')\n",
    "axes[1,0].set_xlabel('x1')\n",
    "axes[1,1].set_xlabel('x2')\n",
    "\n",
    "axes[0,0].set_xlim(-10, 10)\n",
    "axes[0,0].set_ylim(-10, 10)\n",
    "fig.tight_layout()\n",
    "axes[0,1].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the variance in these data is along one particular direction; we might therefore be able to do some analysis of this data by considering just that direction. But in general, that vector might be some combination of the data that you measure. With PCA, we assume that the eigenvectors computed from the empirical covariance matrix describe these directions. If we rank the eigenvectors by the eigenvalues, the directions will be in decreasing importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cov = np.cov(X.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(Cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort by the eigenvalues and project the data onto the eigenbasis. The 0th feature is now the most informative, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = eig_vals.argsort()\n",
    "Y = X[:,idx] @ eig_vecs[idx] # projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1, 2, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "axes[0].plot(Y[:,0], Y[:,1], marker='.', alpha=0.5, linestyle='none')\n",
    "axes[1].plot(Y[:,0], Y[:,2], marker='.', alpha=0.5, linestyle='none')\n",
    "\n",
    "axes[0].set_xlim(-10, 10)\n",
    "axes[0].set_ylim(-10, 10)\n",
    "\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[1].set_xlabel('$x_1$')\n",
    "axes[1].set_ylabel('$x_3$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to quantify how much variance or information is contained along each eigenvector is to compute the _cumulative explained variance_. This tells you, at a given eigenvector, what fraction the total variance is explained by the given eigenvector and all eigenvectors preceding it (with larger eigenvalues). Heres the CEV for the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.cumsum(eig_vals) / np.sum(eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the 0th eigenvector *alone* contains ~98% of the variance of the distribution, and 99.9% is contained with the first two. You have to decide what fraction of the explained variance is OK with you, and truncate the eigenvectors there. Here, if we arbitrarily say we want >95% of the explained variance, we would just keep the first eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Eigenfaces\n",
    "\n",
    "Here we're going to do an example of PCA with some more interesting, high-dimensional data: pictures of faces. Each image is 62 x 47 (why would they do that to us!), so if we treat each pixel as a feature, that's 2914 features *per image* or *per object*. We'll do a PCA on that large feature-space and see how many \"eigenfaces\" we need to explain 95% of the variance (i.e. do a pretty good job at representing faces).\n",
    "\n",
    "_Note: this is heavily inspired by Jake Vanderplas' [notebook]( https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_lfw_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch the face image data\n",
    "faces = fetch_lfw_people(min_faces_per_person=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faces.images.shape, faces.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample of what some of the images look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8, figsize=(9, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(faces.images[i], cmap='Greys_r')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the pixel \"flux\" values as if the are features / dimensions of the data. Here are a few projections of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 2, figsize=(8,8), sharex=True, sharey=True)\n",
    "\n",
    "axes[0,0].scatter(faces.images[:,0,0], faces.images[:,32,16], marker='.')\n",
    "axes[0,1].scatter(faces.images[:,11,21], faces.images[:,25,13], marker='.')\n",
    "axes[1,0].scatter(faces.images[:,13,41], faces.images[:,59,12], marker='.')\n",
    "axes[1,1].scatter(faces.images[:,2,20], faces.images[:,4,11], marker='.')\n",
    "\n",
    "axes[0,0].set_xlim(-5, 260)\n",
    "axes[0,0].set_ylim(-5, 260)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform a PCA on the image data, and keep the largest 256 eigenvectors (which are actually images). Here, we'll use the PCA implementation in `scikit-learn` which actually doesn't compute eigenvectors (it does an SVD on the input data) and is a bit more numerically stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(256)\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the top eigenfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8, figsize=(9, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='Greys_r')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from being slightly terrifying, it's cool! You can see components needed for shading faces, making eyes darker, making noses wider, etc. Let's look at the cumulative explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative variance');\n",
    "plt.ylim(0, 1)\n",
    "plt.xscale('log', basex=2)\n",
    "plt.axvline(faces.images[0].size, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line is at 2914, the number of input features. How many eigenfaces do we need to use to preserve 95% of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmin(np.abs(np.cumsum(pca.explained_variance_ratio_) - 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 170! That's a big data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a few more images to the dataset and then see how they project to smaller numbers of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_faces = []\n",
    "for filename in glob.glob('../data/faces/*.jpg'):\n",
    "    image_file = Image.open(filename)\n",
    "    image = image_file.convert('L')  # convert image to monochrome\n",
    "    image = np.array(image).astype(float)\n",
    "    other_faces.append(image)\n",
    "\n",
    "n_other = len(other_faces)\n",
    "other_faces = np.array(other_faces)\n",
    "other_faces = other_faces.reshape(n_other, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_face_data = np.vstack((faces.data, other_faces))\n",
    "image_shape = faces.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_pca(n_components):\n",
    "    fig, ax = plt.subplots(2, 8, figsize=(16, 6),\n",
    "                           subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "    pca = PCA(n_components)\n",
    "    pca.fit(all_face_data)\n",
    "\n",
    "    for i in range(len(other_faces)):\n",
    "        components = pca.transform(other_faces[i].reshape(1, -1))\n",
    "        projected = pca.inverse_transform(components)\n",
    "        ax.flat[i].imshow(projected[0].reshape(image_shape), cmap='binary_r')\n",
    "\n",
    "    fig.suptitle('{}-dim reconstruction'.format(pca.n_components), fontsize=24)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in [64, 128, 256, 512]:\n",
    "    do_pca(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize where these familiar faces lie in projections of the most informative eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(64)\n",
    "pca.fit(all_face_data)\n",
    "\n",
    "compressed_data = np.zeros((n_other, pca.n_components))\n",
    "for i in range(n_other):\n",
    "    components = pca.transform(other_faces[i].reshape(1, -1))[0]\n",
    "    compressed_data[i] = components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(3, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if j > i: continue\n",
    "        axes[i,j].scatter(compressed_data[:,j], compressed_data[:,i+1], \n",
    "                          c=gender, cmap='coolwarm')\n",
    "\n",
    "fig.tight_layout()\n",
    "        \n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if j > i: axes[i,j].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: What are some of the key limitations to doing this in the Real World?\n",
    "\n",
    "* Uncertainties\n",
    "* Data have to be drawn from a linear subspace of the full spectral space. \n",
    "* Trouble separating amplitude changes (overall flux or luminosity changes) from shape changes\n",
    "* No idea about prior information; it is just as happy creating components with negative amplitudes as positive amplitudes, many spectra have completely unphysical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Eigenspectra\n",
    "\n",
    "Everything that we did above for images can be applied to spectra instead. Run a PCA on the SDSS corrected spectra keeping only the 64 largest eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from astroML.datasets import sdss_corrected_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to normalize the spectra so the mean is at 0 -- we'll add this mean back in later when we reconstruct the spectra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec_mean = spectra.mean(axis=0)\n",
    "spectra -= spec_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "for i in range(8):\n",
    "    plt.plot(wavelengths, spectra[i] + spec_mean, marker='', drawstyle='steps-mid', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(64)\n",
    "pca.fit(spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative explained variance ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative variance');\n",
    "plt.ylim(0, 1)\n",
    "# plt.xscale('log', basex=2)\n",
    "# plt.xlim(2**0, 2**12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a spectrum (e.g., index 100). Plot it, then plot reconstructions using 2, 4, and 8 eigenvectors on separate panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coeff = pca.transform(spectra[4].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Does this look OK to you? Make a plot of the 0th coefficients vs. the 1st coefficients computed by projecting all spectra onto the 0th and 1st eigenspectra. Is there anything troubling about the distribution of points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Masked data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evals = data['evals'] ** 2\n",
    "evals_cs = evals.cumsum()\n",
    "evals_cs /= evals_cs[-1]\n",
    "evecs = data['evecs']\n",
    "spec_mean = spectra.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(evals_cs[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spectra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coeffs = np.einsum('ij,kj->ki', evecs, spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=16)\n",
    "clf.fit(coeffs[:,:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(5, 2, figsize=(14,10), sharex=True)\n",
    "for i,label in enumerate(np.unique(clf.labels_)[:10]):\n",
    "    class_spectra = spectra[clf.labels_==label] + spec_mean\n",
    "    \n",
    "    for k in range(min(len(class_spectra), 8)):\n",
    "        axes.flat[i].plot(wavelengths, class_spectra[k] - np.median(class_spectra[k]), color='k', alpha=0.25,\n",
    "                          marker='', linestyle='-', drawstyle='steps-mid')\n",
    "    \n",
    "    med_y = np.median(class_spectra)\n",
    "    mad_y = np.median(np.abs(class_spectra - med_y))\n",
    "    axes.flat[i].set_ylim(med_y - 8*mad_y, med_y + 8*mad_y)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "teaching/ast542/notebooks/Machine learning - intro - lecture notes-Copy1.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python [conda env:statsseminar]",
   "language": "python",
   "name": "conda-env-statsseminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
