{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('apw-notebook.mplstyle')\n",
    "%matplotlib inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol{\\rm #1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\T}[1]{{#1}^{\\mathsf{T}}}\n",
    "\\newcommand{\\inv}[1]{{#1}^{-1}}\n",
    "$$\n",
    "\n",
    "# Gaussian processes\n",
    "\n",
    "GPs are a type of very flexible models that are primarily useful for prediction, discovery, and making models robust to unknown error properties. A lot of the GP literature is focused around time-series analysis, but they are useful in other contexts as well in which one variable (the analog to 'time') is very precisely measured or perfectly known. They are pretty slow to compute, but people like Dan Foreman-Mackey take speed very seriously (see [george](http://github.com/dfm/george) and [celerite](http://celerite.readthedocs.io/)).\n",
    "\n",
    "The default resource for Gaussian processes is [Rasmussen & Williams](http://www.gaussianprocess.org/gpml/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous classes we talked about the problem of estimating the mean flux of a star given noisy measurements. Recall that we assumed that the data were independent, uncorrelated, and Gaussian, and with those assumptions the likelihood over all data was separable into a product over individual (per–datum) Gaussians. For $N$ data points labeled $n$, we wrote:\n",
    "\n",
    "$$\n",
    "p(\\{f_n\\} \\,|\\, f_0, \\sigma_n^2) = \\prod_n^N \\mathcal{N}(f_n \\,|\\, f_0, \\sigma_n^2) \\\\\n",
    "\\ln p(\\{f_n\\} \\,|\\, f_0, \\sigma_n^2) = -\\frac{1}{2}\\sum_n^N \\left[\\frac{(f_n - f_0)^2}{\\sigma_n^2} + \\ln\\left(2\\pi \\,\\sigma_n^2\\right)\\right]\n",
    "$$\n",
    "\n",
    "Remember that we can also write this log-likelihood using matrix notation by defining a _residual vector_, $\\vec{r} = \\vec{f} - f_0$, and a _covariance matrix_, $\\mat{C}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{f} &= \\begin{bmatrix}\n",
    "             f_{1} \\\\\n",
    "             f_{2} \\\\\n",
    "             \\vdots \\\\\n",
    "             f_{N}\n",
    "           \\end{bmatrix}\\\\\n",
    "\\mat{C} &= \\begin{bmatrix}\n",
    "             \\sigma_1^2 & 0 & \\cdots & 0 \\\\\n",
    "             0 & \\sigma_2^2 & \\cdots & 0  \\\\\n",
    "             \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "             0 & 0 & \\cdots & \\sigma_{N}^2\n",
    "           \\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With these objects, the log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ln p(\\vec{f} \\,|\\, f_0, \\mat{C}) = -\\frac{1}{2} \\T{\\vec{r}} \\, \\inv{\\mat{C}} \\, \\vec{r} \n",
    "    - \\frac{1}{2} \\ln\\det\\mat{C} - \\frac{N}{2}\\ln 2\\pi\n",
    "$$\n",
    "\n",
    "Writing it like this you can interpret the data vector $\\vec{f}$ as being a single draw from an $N$-dimensional Gaussian with mean $f_0$ and (diagonal) covariance matrix $\\mat{C}$. The diagonality of the covariance matrix is a result of our assumption that the data are uncorrelated or independent. But what if the data are correlated in some way? Or, what if there are off-diagonal elements in the covariance matrix? Let's look at both cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for simplicity, set f_0 to 0\n",
    "f0 = 0.\n",
    "n_data = 64\n",
    "\n",
    "times = np.random.uniform(0, 100, n_data)\n",
    "times.sort()\n",
    "\n",
    "y_err = np.random.uniform(0.1, 1., n_data)\n",
    "\n",
    "# first construct a purely diagonal covariance matrix\n",
    "C1 = np.diag(y_err**2)\n",
    "\n",
    "# now create a covariance matrix with correlated noise\n",
    "C2 = 8 * np.exp(-0.5 * (times[:,None]-times[None,:])**2 / 10.**2)\n",
    "C2[np.diag_indices_from(C2)] += y_err**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 1, figsize=(5,10))\n",
    "axes[0].imshow(C1, cmap='Greys_r')\n",
    "axes[1].imshow(C2, cmap='Greys_r')\n",
    "for ax in axes:\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = np.random.multivariate_normal(np.full(n_data, f0), C1)\n",
    "f2 = np.random.multivariate_normal(np.full(n_data, f0), C2)\n",
    "\n",
    "fig,axes = plt.subplots(2, 1, figsize=(8,8), sharex=True, sharey=True)\n",
    "\n",
    "axes[0].errorbar(times, f1, yerr=y_err, marker='.', linestyle='none')\n",
    "axes[1].errorbar(times, f2, yerr=y_err, marker='.', linestyle='none')\n",
    "\n",
    "axes[0].set_ylim(-8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we knew this covariance matrix for the data, then doing least-squares fitting would be as simple as ever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_linalg(times, flux, Cov):\n",
    "    # construct design matrix and invert covariance matrix\n",
    "    X = np.vander(times, N=1)\n",
    "    Cinv = np.linalg.inv(Cov)\n",
    "\n",
    "    best_f0 = np.linalg.inv(X.T @ Cinv @ X) @ (X.T @ Cinv @ flux)\n",
    "    cov_f0 = np.linalg.inv(X.T @ Cinv @ X)\n",
    "    \n",
    "    return best_f0, cov_f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.linspace(times.min(), times.max(), 1024)\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "ax.errorbar(times, f2, yerr=y_err, marker='.', linestyle='none')\n",
    "\n",
    "for color, C in zip(['#31a354', '#de2d26'], [C2, C1]):\n",
    "    best_f0, cov_f0 = solve_linalg(times, f2, C)\n",
    "    samples = np.random.multivariate_normal(best_f0, cov_f0, 10000)\n",
    "\n",
    "    lines = np.dot(np.vander(x0, 1), samples.T)\n",
    "    q = np.percentile(lines, [16, 84], axis=1)\n",
    "\n",
    "    ax.plot(x0, np.full_like(x0, best_f0), linewidth=2, alpha=0.8, color=color)\n",
    "    ax.fill_between(x0, q[0], q[1], alpha=0.25, color=color)\n",
    "\n",
    "ax.set_ylim(-8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we don't know the noise structure? For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "data = fits.getdata('../data/k2.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[np.isfinite(data['PDCSAP_FLUX'])]\n",
    "raw_time = data['TIME'].astype(np.float64)\n",
    "raw_flux = data['PDCSAP_FLUX'].astype(np.float64)\n",
    "raw_flux_err = data['PDCSAP_FLUX_ERR'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "med = np.median(raw_flux)\n",
    "robust_std = 1.48 * np.median(np.abs(raw_flux - med))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = (raw_flux < (med + 5*robust_std)) & (raw_flux > (med - 5*robust_std))\n",
    "\n",
    "trim = 512\n",
    "time = raw_time[idx][:trim] # too much data\n",
    "flux = raw_flux[idx][:trim]\n",
    "flux_err = raw_flux_err[idx][:trim]\n",
    "\n",
    "time = time - time.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.errorbar(time, flux, flux_err, marker='.', linestyle='none')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only given single error-bars for each flux measurement, i.e. just the diagonal terms of the covariance matrix. But there is likely some very correlated noise in this light curve! What do we do? One (very bad) idea would be to add all off-diagonal elements to your model as parameters! For an $N\\times N$ symmetric matrix, that amounts to adding $\\frac{N\\,(N-1)}{2}$ parameters to your model (and scales like the number of data points _squared_!). No one would seriously consider doing that...\n",
    "\n",
    "Instead: what if we can write a simple parametrized _kernel function_, $k$, that generates the off-diagonal elements as a function of location in the matrix? That is:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sigma_i^2 \\, \\delta_{ij} + k(t_i, t_j)\n",
    "$$\n",
    "\n",
    "We now just have to specify this function, $k$. This is where the magic comes in...the function must produce a symmetric, positive, semi-definite function, so there aren't too many options. We'll introduce a few others later, but for now let's use the _Squared Exponential Kernel_:\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = a^2 \\, \\exp\\left(-\\frac{(x_i - x_j)^2}{2\\,b^2}\\right)\n",
    "$$\n",
    "\n",
    "(yes it's just another Gaussian...). This kernel says that data points near $x_i$ are correlated with a length scale set by the parameter $b$ and a correlation amplitude set by the parameter $a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Implement this kernel function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sq_exp_kernel(x_i, x_j, a, b):\n",
    "    # TODO: Delete the line below and fill in this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 4.\n",
    "b = 1.\n",
    "Cov = sq_exp_kernel(time[:,None], time[None], a, b)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(Cov, cmap='Greys_r')\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.gca().yaxis.set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now implement a log-likelihood function for a Gaussian process with a simple constant mean model (same as we did above for inferring the mean flux):\n",
    "\n",
    "$$\n",
    "\\ln p(\\vec{f} \\,|\\, f_0, \\mat{C}) = -\\frac{1}{2} \\T{\\vec{r}} \\, \\inv{\\mat{C}} \\, \\vec{r} \n",
    "    - \\frac{1}{2} \\ln\\det\\mat{C} - \\frac{N}{2}\\ln 2\\pi\n",
    "$$\n",
    "\n",
    "Replacing $\\vec{C}$ with the sum of the diagonal covariance matrix with that generated by the kernel function.\n",
    "\n",
    "The parameters that come in will be $(f_0, \\ln a, \\ln b)$. You'll need to invert a large matrix in the likelihood call. Rather than using `numpy.linalg.inv`, I recommend using `numpy.linalg.solve` in the following way. As a part of the likelihood expression, you need to compute the vector $\\vec{y}$ in:\n",
    "\n",
    "$$\n",
    "\\vec{y} = \\inv{\\mat{C}}\\,\\vec{r}\\\\\n",
    "\\vec{r} = \\mat{C}\\,\\vec{y}\n",
    "$$\n",
    "\n",
    "Instead of explicitly doing the inversion, you can do:\n",
    "$$\n",
    "\\vec{y} = {\\rm solve}\\left(\\mat{C}, \\vec{r}\\right)\n",
    "$$\n",
    "\n",
    "Also, instead of taking the log of the determinant, use `numpy.linalg.slogdet` (which is much more numerically stable).\n",
    "\n",
    "__Exercise__: Implement the likelihood function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_likelihood(pars, t, flux, flux_err):   \n",
    "    # TODO: Delete the line below and fill in this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to implement priors for the parameters. We'll assume a uniform prior on $f_0$ and uniform priors on the log hyper-parameters $\\ln a$ and $\\ln b$. Choose wide and sensible ranges for the parameters.\n",
    "\n",
    "__Exercise:__ Implement the prior function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_prior(pars):\n",
    "    # TODO: Delete the line below and fill in this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run MCMC to infer the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_posterior(pars, t, flux, flux_err):\n",
    "    lp = ln_prior(pars)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    \n",
    "    ll = ln_likelihood(pars, t, flux, flux_err)\n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    \n",
    "    return lp + ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nwalkers = 32\n",
    "p0 = [np.median(flux), 4., -2.]\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, dim=len(p0), lnpostfn=ln_posterior,\n",
    "                                args=(time, flux, flux_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning__: this could take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_p0 = emcee.utils.sample_ball(p0, std=[1E-3]*len(p0), size=nwalkers)\n",
    "\n",
    "pos,_,_ = sampler.run_mcmc(_p0, 256)\n",
    "sampler.reset()\n",
    "_ = sampler.run_mcmc(pos, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(len(p0)):\n",
    "    plt.figure()\n",
    "    for walker in sampler.chain[...,k]:\n",
    "        plt.plot(walker, marker='', drawstyle='steps-mid', color='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [@dfm](https://github.com/dfm)'s Gaussian process package [George](https://github.com/dfm/george) to plot samples from the GP at each choice of the hyper-parameters $a, b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import george"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gp(grid, time, flux, flux_err, f0, a, b):\n",
    "    K = a**2 * george.kernels.ExpSquaredKernel(b)\n",
    "    gp = george.GP(K, mean=f0)\n",
    "    gp.compute(time, flux_err)\n",
    "    return gp.predict(flux, grid, mean_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.errorbar(time, flux, flux_err, marker='.', linestyle='none')\n",
    "\n",
    "t_grid = np.linspace(time.min(), time.max(), 1024)\n",
    "flatchain = np.vstack(sampler.chain[:,::32])\n",
    "\n",
    "for i in np.random.choice(flatchain.shape[0], size=64, replace=False):\n",
    "    f0,ln_a,ln_b = flatchain[i]\n",
    "    mean = plot_gp(t_grid, time, flux, flux_err, f0, np.exp(ln_a), np.exp(ln_b))    \n",
    "    plt.plot(t_grid, mean, marker='', zorder=100, color='#2b8cbe', alpha=0.25)\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how they work, let's use `George` to compute the likelihood (it's much faster!) and re-do the analysis above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from george import kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_likelihood2(pars, t, flux, flux_err):       \n",
    "    f_0 = pars[0]\n",
    "    a, b = np.exp(pars[1:])\n",
    "    \n",
    "    # residual vector - you could replace with a more complicated mean model\n",
    "    r = flux - f_0 \n",
    "    \n",
    "    K = a**2 * kernels.ExpSquaredKernel(b)\n",
    "    gp = george.GP(K)\n",
    "    gp.compute(t, flux_err)\n",
    "    return gp.lnlikelihood(flux - f_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_prior2(pars):    \n",
    "    f_0 = pars[0]\n",
    "    \n",
    "    if f_0 < 0 or f_0 > 1E10:\n",
    "        return -np.inf\n",
    "    \n",
    "    for ln_p in pars[1:]:\n",
    "        if ln_p < -10 or ln_p > 10:\n",
    "            return -np.inf\n",
    "    \n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_posterior2(pars, t, flux, flux_err):\n",
    "    lp = ln_prior2(pars)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    \n",
    "    ll = ln_likelihood2(pars, t, flux, flux_err)\n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    \n",
    "    return lp + ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nwalkers = 32\n",
    "p0 = [np.median(flux), 4., -2.]\n",
    "\n",
    "sampler2 = emcee.EnsembleSampler(nwalkers, dim=len(p0), lnpostfn=ln_posterior2,\n",
    "                                 args=(time, flux, flux_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_p0 = emcee.utils.sample_ball(p0, std=[1E-3]*len(p0), size=nwalkers)\n",
    "\n",
    "pos,_,_ = sampler2.run_mcmc(_p0, 256)\n",
    "sampler2.reset()\n",
    "_ = sampler2.run_mcmc(pos, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(len(p0)):\n",
    "    plt.figure()\n",
    "    for walker in sampler2.chain[...,k]:\n",
    "        plt.plot(walker, marker='', drawstyle='steps-mid', color='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatchain = np.vstack(sampler2.chain[:,::32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.errorbar(time, flux, flux_err, marker='.', linestyle='none')\n",
    "\n",
    "t_grid = np.linspace(time.min(), time.max(), 1024)\n",
    "flatchain = np.vstack(sampler2.chain[:,::32])\n",
    "\n",
    "for i in np.random.choice(flatchain.shape[0], size=64, replace=False):\n",
    "    f0, ln_a, ln_b = flatchain[i]\n",
    "    a = np.exp(ln_a)\n",
    "    b = np.exp(ln_b)\n",
    "    \n",
    "    K = a**2 * kernels.ExpSquaredKernel(b)\n",
    "    gp = george.GP(K, mean=f0)\n",
    "    gp.compute(time, flux_err)\n",
    "    mean = gp.predict(flux, t_grid, mean_only=True)\n",
    "    plt.plot(t_grid, mean, marker='', zorder=100, color='#2b8cbe', alpha=0.25)\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been using an exponential-squared kernel function but I never really justified its use. That's a generic problem of GP models: there are a handful of popular kernel functions, but sometimes there's no real motivation to use one over another. In this case, however, the variability in our data (the \"noise\") clearly has contributions over a large range of frequencies. The kernel we're using has a fixed correlation length-scale, meaning it won't necessarily adapt well when there are sharp features and long-term trends. \n",
    "\n",
    "__Exercise__: Re-do the analysis (using `George`) we did above but instead with the following kernel, known as the \"Matérn 3/2\":\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = a^2 \\, \\left(1 + \\frac{\\sqrt{3}\\,\\left|x_i-x_j\\right|}{b}\\right) \\, \\exp\\left(-\\frac{\\sqrt{3}\\,\\left|x_i-x_j\\right|}{b}\\right)\n",
    "$$\n",
    "\n",
    "(see `george.kernels.Matern32Kernel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:statsseminar]",
   "language": "python",
   "name": "conda-env-statsseminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
